{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0da3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a0f4e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create known paramters\n",
    "# creating data\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# data\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "# x is capital because it's a matrix or tensor\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)  # adds an extra dimension\n",
    "y = weight * X + bias\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe0088ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10, 40, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a train/test split\n",
    "# 80 percent for the train split\n",
    "train_split = int(0.8 * len(X))\n",
    "# first 40 examples for the training examples\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "# last 10 examples for the testing examples\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "X_train, y_train = X_train.to(device=\"cuda\"), y_train.to(device=\"cuda\")\n",
    "X_test, y_test = X_test.to(device=\"cuda\"), y_test.to(device=\"cuda\")\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5a4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(\n",
    "            in_features=1, out_features=10\n",
    "        )  # Change in_features to 1\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=40)\n",
    "        self.layer_3 = nn.Linear(in_features=40, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "439e6bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer_1.weight',\n",
       "              tensor([[-0.8321],\n",
       "                      [-0.8196],\n",
       "                      [ 0.6730],\n",
       "                      [-0.8351],\n",
       "                      [-0.5849],\n",
       "                      [-0.8847],\n",
       "                      [-0.5796],\n",
       "                      [ 0.9419],\n",
       "                      [ 0.5566],\n",
       "                      [-0.3973]], device='cuda:0')),\n",
       "             ('layer_1.bias',\n",
       "              tensor([-0.6293, -0.5479, -0.8057, -0.8502,  0.8267, -0.1019,  0.0563,  0.7802,\n",
       "                      -0.0946,  0.3353], device='cuda:0')),\n",
       "             ('layer_2.weight',\n",
       "              tensor([[-0.0983,  0.2223, -0.2431, -0.0055,  0.0044, -0.1990, -0.0385,  0.1942,\n",
       "                       -0.1578, -0.0012],\n",
       "                      [-0.2262,  0.0779,  0.2105,  0.0412,  0.1396, -0.1145, -0.0856, -0.3069,\n",
       "                       -0.1866, -0.2504],\n",
       "                      [-0.2506, -0.0025,  0.0153,  0.1535,  0.1239,  0.2695,  0.2223,  0.3098,\n",
       "                        0.2690, -0.1970],\n",
       "                      [-0.0742,  0.1041, -0.0797, -0.0974,  0.2547,  0.1891,  0.2026,  0.3055,\n",
       "                       -0.3047,  0.0396],\n",
       "                      [ 0.0641, -0.2394, -0.2680,  0.1172, -0.0238,  0.0442,  0.1558, -0.0438,\n",
       "                        0.3075, -0.1706],\n",
       "                      [ 0.0631,  0.0299, -0.1325, -0.1642,  0.2609,  0.2646,  0.2044,  0.0819,\n",
       "                       -0.1302,  0.1360],\n",
       "                      [ 0.1499, -0.0140,  0.2959, -0.2667,  0.2292, -0.0026,  0.2068,  0.1708,\n",
       "                       -0.0737, -0.1135],\n",
       "                      [-0.1076, -0.0443, -0.0410, -0.2862,  0.1594, -0.1017,  0.1316,  0.2831,\n",
       "                        0.1648, -0.0741],\n",
       "                      [-0.0761, -0.0321,  0.1600, -0.0875, -0.1625,  0.2532, -0.0246, -0.2209,\n",
       "                       -0.1614,  0.0323],\n",
       "                      [ 0.2185, -0.3088, -0.2420, -0.3115, -0.1092,  0.2613, -0.1985, -0.1182,\n",
       "                        0.1793, -0.0200],\n",
       "                      [-0.2181, -0.2410, -0.2668, -0.1056,  0.2941,  0.1628, -0.2682,  0.0797,\n",
       "                        0.0269, -0.0377],\n",
       "                      [ 0.1789,  0.3027,  0.0998, -0.0610,  0.2868, -0.1024, -0.0381, -0.1590,\n",
       "                       -0.0312,  0.0750],\n",
       "                      [ 0.0583,  0.1424, -0.1409,  0.0274,  0.3095,  0.2450,  0.0991, -0.2192,\n",
       "                        0.0388, -0.0434],\n",
       "                      [-0.2186,  0.2239, -0.0885, -0.0216,  0.0676,  0.3078,  0.1461,  0.0017,\n",
       "                       -0.2680, -0.0658],\n",
       "                      [-0.1485,  0.3112,  0.1980, -0.2760, -0.0835, -0.0490, -0.0694, -0.0383,\n",
       "                       -0.1370, -0.2083],\n",
       "                      [-0.1929, -0.1134, -0.2151, -0.1983,  0.1095, -0.0384, -0.2112,  0.0760,\n",
       "                        0.0872, -0.0384],\n",
       "                      [ 0.0265, -0.2431, -0.0990, -0.1110, -0.2233, -0.2847, -0.2934,  0.0668,\n",
       "                       -0.1929, -0.0035],\n",
       "                      [ 0.0536,  0.1443,  0.3094,  0.1454,  0.0893, -0.2754,  0.0039,  0.0444,\n",
       "                        0.2969,  0.2714],\n",
       "                      [-0.1646, -0.2632,  0.1101,  0.1548, -0.1989,  0.1928,  0.0885,  0.0341,\n",
       "                       -0.0353, -0.0854],\n",
       "                      [ 0.0121,  0.0541,  0.0417,  0.2596,  0.2803, -0.1733, -0.0729,  0.0523,\n",
       "                        0.3040, -0.2990],\n",
       "                      [ 0.2791,  0.1839,  0.2529, -0.2999,  0.1230,  0.2779,  0.1581, -0.2675,\n",
       "                        0.2921,  0.2025],\n",
       "                      [ 0.0195, -0.2164,  0.1321, -0.0310, -0.0508, -0.1516,  0.2921, -0.0638,\n",
       "                       -0.0919, -0.1807],\n",
       "                      [-0.1331,  0.1330,  0.0272,  0.2212,  0.0248, -0.0594,  0.1468, -0.1934,\n",
       "                        0.2108,  0.0450],\n",
       "                      [-0.1885, -0.0418, -0.1343, -0.0027,  0.2908,  0.1685,  0.0823,  0.0818,\n",
       "                       -0.1642, -0.1504],\n",
       "                      [-0.2231, -0.3022, -0.1585, -0.0486,  0.0957,  0.1275,  0.2922, -0.1005,\n",
       "                        0.2896,  0.1046],\n",
       "                      [ 0.0284,  0.1602,  0.2578, -0.0824, -0.1036, -0.1681, -0.0575, -0.0865,\n",
       "                       -0.1274, -0.0320],\n",
       "                      [-0.1592, -0.2385, -0.2024,  0.0164,  0.1412, -0.2828,  0.2568,  0.1177,\n",
       "                        0.1355, -0.1467],\n",
       "                      [-0.3044,  0.2699,  0.1400, -0.1171, -0.2151,  0.0260,  0.0771,  0.1240,\n",
       "                        0.0547,  0.1822],\n",
       "                      [ 0.0390, -0.0078,  0.2544,  0.0351,  0.2945, -0.0256, -0.0241,  0.2780,\n",
       "                        0.2322, -0.0200],\n",
       "                      [ 0.0201, -0.1232,  0.2297,  0.2254,  0.2986,  0.0197,  0.2553, -0.3032,\n",
       "                       -0.2408, -0.2447],\n",
       "                      [ 0.1699, -0.2515,  0.0234, -0.0014, -0.0093,  0.2588, -0.1388, -0.2076,\n",
       "                       -0.2765, -0.2046],\n",
       "                      [ 0.0246,  0.2628, -0.2516,  0.1062, -0.3014, -0.0254, -0.1051, -0.0067,\n",
       "                       -0.2371, -0.2300],\n",
       "                      [-0.0188, -0.1051,  0.0943, -0.2926, -0.1098,  0.0320, -0.3091, -0.3040,\n",
       "                       -0.0244, -0.1925],\n",
       "                      [-0.2347, -0.1453,  0.0058, -0.0210, -0.2008,  0.0177,  0.1978, -0.1963,\n",
       "                        0.2767,  0.1217],\n",
       "                      [ 0.2755,  0.2902, -0.2478, -0.0319,  0.0118, -0.1386,  0.0630,  0.0575,\n",
       "                       -0.2919,  0.0810],\n",
       "                      [ 0.2361,  0.0305, -0.3114, -0.0373,  0.1864, -0.1727,  0.1812, -0.0381,\n",
       "                        0.0930,  0.2131],\n",
       "                      [ 0.2690,  0.3078,  0.1969,  0.0148,  0.2661,  0.1565,  0.2623, -0.0255,\n",
       "                       -0.2247, -0.0840],\n",
       "                      [ 0.1911,  0.1355,  0.3026,  0.3047, -0.1381, -0.2983, -0.2757, -0.2092,\n",
       "                        0.0819, -0.0949],\n",
       "                      [-0.0849,  0.1974, -0.0326,  0.2448,  0.0121, -0.0952, -0.2409,  0.2948,\n",
       "                       -0.2629, -0.0311],\n",
       "                      [-0.0595, -0.2578,  0.1638,  0.1216, -0.2960,  0.1720,  0.2378,  0.2196,\n",
       "                        0.1099,  0.0207]], device='cuda:0')),\n",
       "             ('layer_2.bias',\n",
       "              tensor([-0.1345, -0.1091, -0.1160,  0.0525, -0.1641,  0.2716,  0.0836,  0.1555,\n",
       "                       0.3074, -0.0008,  0.3030, -0.0196,  0.0556, -0.2353,  0.1696, -0.2341,\n",
       "                       0.1213, -0.1624,  0.0851,  0.0173,  0.1537,  0.2058, -0.1928,  0.2921,\n",
       "                       0.1117,  0.0578,  0.1987, -0.0882, -0.0101, -0.2359,  0.1624,  0.1308,\n",
       "                       0.0244,  0.2028,  0.1171,  0.0006,  0.0278, -0.2014, -0.0778, -0.0486],\n",
       "                     device='cuda:0')),\n",
       "             ('layer_3.weight',\n",
       "              tensor([[ 0.0888,  0.0010,  0.1098, -0.0268, -0.1193,  0.0562,  0.0565,  0.0181,\n",
       "                       -0.1243,  0.1507, -0.1233, -0.0493, -0.1557,  0.0697,  0.0200,  0.1167,\n",
       "                        0.1035,  0.0242, -0.1498, -0.0956,  0.1408, -0.1193, -0.0200, -0.1351,\n",
       "                        0.1017,  0.1013, -0.0809, -0.0669,  0.1520,  0.0980, -0.1362,  0.1223,\n",
       "                        0.1356,  0.0352,  0.0401,  0.0885, -0.0174, -0.0350,  0.1530, -0.1500],\n",
       "                      [-0.0290, -0.0075,  0.0051, -0.0622, -0.0610,  0.1451,  0.1469, -0.0644,\n",
       "                        0.0419, -0.0821, -0.1288,  0.0094, -0.0051,  0.1202,  0.1288, -0.0527,\n",
       "                        0.0694,  0.0581,  0.0134, -0.0228,  0.1384,  0.0945, -0.0460,  0.0210,\n",
       "                        0.0503,  0.1531, -0.1510,  0.0414, -0.1509,  0.0604,  0.0599,  0.1149,\n",
       "                        0.0155, -0.0668, -0.1312, -0.0026,  0.0210,  0.0018,  0.0086, -0.0266],\n",
       "                      [ 0.0384, -0.1066, -0.1105, -0.1098,  0.1242, -0.0580, -0.0262,  0.1334,\n",
       "                       -0.0849, -0.1350,  0.0126, -0.1218,  0.1441, -0.1047, -0.1074, -0.0687,\n",
       "                       -0.1171, -0.1213,  0.1433, -0.0007, -0.0205,  0.0518, -0.0831,  0.0227,\n",
       "                        0.1302,  0.0903,  0.0860, -0.1477,  0.1277, -0.0050, -0.0307, -0.0213,\n",
       "                       -0.0545, -0.0945,  0.1146, -0.0442,  0.0862, -0.1203, -0.1043,  0.0078],\n",
       "                      [-0.1151, -0.0794, -0.1173, -0.0355,  0.0047,  0.0395,  0.1068,  0.0569,\n",
       "                        0.0720, -0.1227,  0.1185, -0.0530,  0.0984,  0.1430, -0.0331, -0.0610,\n",
       "                        0.1388, -0.0564, -0.0588, -0.0750, -0.0004,  0.0975, -0.1031,  0.0258,\n",
       "                       -0.0982, -0.0826, -0.0467,  0.0903,  0.1158, -0.1069,  0.1408,  0.0027,\n",
       "                       -0.1311, -0.1381,  0.1392, -0.0699, -0.0424,  0.1114,  0.0704,  0.0310],\n",
       "                      [ 0.1260,  0.0754, -0.0857, -0.0043,  0.1070,  0.1426,  0.1340, -0.0834,\n",
       "                       -0.0232, -0.1179,  0.0284, -0.1048, -0.1466,  0.1569, -0.0721, -0.1368,\n",
       "                       -0.0762,  0.0410,  0.1241,  0.0979,  0.0189,  0.1159,  0.0076, -0.1521,\n",
       "                       -0.0358,  0.0953,  0.1051,  0.1093,  0.0436, -0.0022, -0.1318, -0.0052,\n",
       "                        0.1041,  0.0399, -0.0542,  0.0922, -0.1076, -0.1386, -0.1337, -0.0037],\n",
       "                      [-0.0893, -0.0745, -0.0455, -0.1574,  0.0602,  0.0945,  0.0394,  0.1418,\n",
       "                       -0.0068, -0.1233, -0.0258, -0.0348,  0.1472, -0.0256,  0.1202, -0.1407,\n",
       "                       -0.0786,  0.0138,  0.0289,  0.0014,  0.0293,  0.0192,  0.0858, -0.1441,\n",
       "                        0.0077, -0.1497,  0.0006,  0.1460, -0.0457, -0.0922,  0.1196, -0.1385,\n",
       "                       -0.0175, -0.1139, -0.0983,  0.1018,  0.0916, -0.0772,  0.0855,  0.1162],\n",
       "                      [-0.1019, -0.0960,  0.0725,  0.0660,  0.0872,  0.1275, -0.0908,  0.0435,\n",
       "                        0.1540,  0.0872,  0.1390, -0.1227, -0.0712, -0.1244, -0.0120, -0.0256,\n",
       "                       -0.0775, -0.1253,  0.0440,  0.0473, -0.1065,  0.1433,  0.1413,  0.0882,\n",
       "                       -0.1146, -0.0357, -0.0333,  0.0371, -0.0992, -0.1463, -0.0672, -0.0281,\n",
       "                        0.0230,  0.0503, -0.0165,  0.0144,  0.1056,  0.0651, -0.1580, -0.0788],\n",
       "                      [-0.0559,  0.0521, -0.0880, -0.0965,  0.0747,  0.0848,  0.0593,  0.0456,\n",
       "                        0.0490, -0.1392, -0.1145,  0.0836,  0.0771, -0.0093, -0.1559, -0.1232,\n",
       "                       -0.0206,  0.1266,  0.0989,  0.0868,  0.1200, -0.1565, -0.0160, -0.1092,\n",
       "                       -0.1031,  0.1034,  0.0469,  0.1023,  0.1474, -0.0306, -0.0779, -0.0147,\n",
       "                       -0.0272,  0.0521, -0.0365,  0.1317,  0.1527,  0.1121,  0.0604,  0.1027],\n",
       "                      [-0.0698, -0.0807,  0.1053,  0.0424, -0.0921, -0.0231,  0.1448,  0.0364,\n",
       "                        0.0733,  0.1012, -0.0893,  0.0298,  0.0905,  0.0782,  0.0550, -0.1489,\n",
       "                       -0.1043, -0.1193, -0.1424,  0.1080, -0.1131, -0.0114,  0.1490,  0.0156,\n",
       "                        0.0624, -0.0568,  0.1090,  0.1306, -0.0436,  0.0742, -0.0030, -0.1270,\n",
       "                        0.1266, -0.0513, -0.0412,  0.0544, -0.1467, -0.0448,  0.0532,  0.0340],\n",
       "                      [-0.1230, -0.0167, -0.0750, -0.0605,  0.0863,  0.1580, -0.1011,  0.0077,\n",
       "                        0.1493, -0.0578,  0.0200,  0.0409, -0.0888,  0.0373, -0.0459,  0.1468,\n",
       "                       -0.0751,  0.1520,  0.1199,  0.0908,  0.1041, -0.0985, -0.1078, -0.0181,\n",
       "                        0.0542,  0.1247,  0.0623, -0.0465, -0.0658, -0.1003,  0.1468,  0.0401,\n",
       "                       -0.1187, -0.1470, -0.1110, -0.0778,  0.0534,  0.1022, -0.1277,  0.1580]],\n",
       "                     device='cuda:0')),\n",
       "             ('layer_3.bias',\n",
       "              tensor([ 0.0678,  0.0916,  0.1232,  0.1459, -0.0967, -0.0635, -0.1282, -0.1267,\n",
       "                      -0.0334, -0.0107], device='cuda:0'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = CircleModelV1().to(device=\"cuda\")\n",
    "model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7d508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfe44bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ae4a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train Loss: 0.60836 | Test Loss: 0.95899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:128: UserWarning: Using a target size (torch.Size([40, 1])) that is different to the input size (torch.Size([40, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "d:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:128: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Train Loss: 0.08133 | Test Loss: 0.19087\n",
      "Epoch: 200 | Train Loss: 0.05023 | Test Loss: 0.11448\n",
      "Epoch: 300 | Train Loss: 0.02299 | Test Loss: 0.04813\n",
      "Epoch: 400 | Train Loss: 0.01214 | Test Loss: 0.01740\n",
      "Epoch: 500 | Train Loss: 0.01104 | Test Loss: 0.00937\n",
      "Epoch: 600 | Train Loss: 0.01124 | Test Loss: 0.00838\n",
      "Epoch: 700 | Train Loss: 0.01140 | Test Loss: 0.00754\n",
      "Epoch: 800 | Train Loss: 0.01165 | Test Loss: 0.00660\n",
      "Epoch: 900 | Train Loss: 0.01115 | Test Loss: 0.00678\n",
      "Epoch: 1000 | Train Loss: 0.01170 | Test Loss: 0.00550\n",
      "Epoch: 1100 | Train Loss: 0.01210 | Test Loss: 0.00431\n",
      "Epoch: 1200 | Train Loss: 0.01220 | Test Loss: 0.00361\n",
      "Epoch: 1300 | Train Loss: 0.01196 | Test Loss: 0.00326\n",
      "Epoch: 1400 | Train Loss: 0.01020 | Test Loss: 0.00347\n",
      "Epoch: 1500 | Train Loss: 0.01022 | Test Loss: 0.00278\n",
      "Epoch: 1600 | Train Loss: 0.01041 | Test Loss: 0.00233\n",
      "Epoch: 1700 | Train Loss: 0.01016 | Test Loss: 0.00227\n",
      "Epoch: 1800 | Train Loss: 0.01008 | Test Loss: 0.00229\n",
      "Epoch: 1900 | Train Loss: 0.01008 | Test Loss: 0.00181\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_1.train()\n",
    "    \n",
    "    y_pred = model_1(X_train).squeeze()\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_1(X_test).squeeze()\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch} | Train Loss: {loss:.5f} | Test Loss: {test_loss:.5f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6bc188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(\n",
    "    train_data=X_train,\n",
    "    train_labels=y_train,\n",
    "    test_data=X_test,\n",
    "    test_labels=y_test,\n",
    "    predictions=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    plots training data, test data and compares predictions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    # plot training data in blue\n",
    "    plt.scatter(\n",
    "        train_data.cpu().numpy(), train_labels, c=\"b\", s=4, label=\"Training data \"\n",
    "    )\n",
    "\n",
    "    # plot testing data in green\n",
    "    plt.scatter(test_data.cpu().numpy(), test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "    # Are there predictions ?\n",
    "    if predictions is not None:\n",
    "        # plot the predictions\n",
    "        plt.scatter(\n",
    "            test_data.cpu().numpy(), predictions, c=\"r\", s=4, label=\"Predictions\"\n",
    "        )\n",
    "\n",
    "    # show the legend\n",
    "    plt.legend(prop={\"size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffa6d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mplot_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mplot_predictions\u001b[39m\u001b[34m(train_data, train_labels, test_data, test_labels, predictions)\u001b[39m\n\u001b[32m     11\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m7\u001b[39m))\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# plot training data in blue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining data \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# plot testing data in green\u001b[39;00m\n\u001b[32m     18\u001b[39m plt.scatter(test_data.cpu().numpy(), test_labels, c=\u001b[33m\"\u001b[39m\u001b[33mg\u001b[39m\u001b[33m\"\u001b[39m, s=\u001b[32m4\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mTesting data\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\pyplot.py:3937\u001b[39m, in \u001b[36mscatter\u001b[39m\u001b[34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, data, **kwargs)\u001b[39m\n\u001b[32m   3917\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.scatter)\n\u001b[32m   3918\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscatter\u001b[39m(\n\u001b[32m   3919\u001b[39m     x: \u001b[38;5;28mfloat\u001b[39m | ArrayLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3935\u001b[39m     **kwargs,\n\u001b[32m   3936\u001b[39m ) -> PathCollection:\n\u001b[32m-> \u001b[39m\u001b[32m3937\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3939\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3940\u001b[39m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3947\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3949\u001b[39m \u001b[43m        \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[43m=\u001b[49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3952\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3953\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3954\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3955\u001b[39m     sci(__ret)\n\u001b[32m   3956\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:4924\u001b[39m, in \u001b[36mAxes.scatter\u001b[39m\u001b[34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, **kwargs)\u001b[39m\n\u001b[32m   4922\u001b[39m edgecolors = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33medgecolor\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   4923\u001b[39m \u001b[38;5;66;03m# Process **kwargs to handle aliases, conflicts with explicit kwargs:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m x, y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_unit_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4925\u001b[39m \u001b[38;5;66;03m# np.ma.ravel yields an ndarray, not a masked array,\u001b[39;00m\n\u001b[32m   4926\u001b[39m \u001b[38;5;66;03m# unless its argument is a masked array.\u001b[39;00m\n\u001b[32m   4927\u001b[39m x = np.ma.ravel(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_base.py:2647\u001b[39m, in \u001b[36m_AxesBase._process_unit_info\u001b[39m\u001b[34m(self, datasets, kwargs, convert)\u001b[39m\n\u001b[32m   2645\u001b[39m     \u001b[38;5;66;03m# Update from data if axis is already set but no unit is set yet.\u001b[39;00m\n\u001b[32m   2646\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis.have_units():\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m         \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis_name, axis \u001b[38;5;129;01min\u001b[39;00m axis_map.items():\n\u001b[32m   2649\u001b[39m     \u001b[38;5;66;03m# Return if no axis is set.\u001b[39;00m\n\u001b[32m   2650\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\axis.py:1745\u001b[39m, in \u001b[36mAxis.update_units\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1739\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1740\u001b[39m \u001b[33;03mIntrospect *data* for units converter and update the\u001b[39;00m\n\u001b[32m   1741\u001b[39m \u001b[33;03m``axis.get_converter`` instance if necessary. Return *True*\u001b[39;00m\n\u001b[32m   1742\u001b[39m \u001b[33;03mif *data* is registered for unit conversion.\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._converter_is_explicit:\n\u001b[32m-> \u001b[39m\u001b[32m1745\u001b[39m     converter = \u001b[43mmunits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1747\u001b[39m     converter = \u001b[38;5;28mself\u001b[39m._converter\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\units.py:167\u001b[39m, in \u001b[36mRegistry.get_converter\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the converter interface instance for *x*, or None.\"\"\"\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Unpack in case of e.g. Pandas or xarray object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m x = \u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_unpack_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.ndarray):\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# In case x in a masked array, access the underlying data (only its\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# type matters).  If x is a regular ndarray, getdata() just returns\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# the array itself.\u001b[39;00m\n\u001b[32m    173\u001b[39m     x = np.ma.getdata(x).ravel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\matplotlib\\cbook.py:2371\u001b[39m, in \u001b[36m_unpack_to_numpy\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   2365\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m xtmp\n\u001b[32m   2366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_array(x) \u001b[38;5;129;01mor\u001b[39;00m _is_jax_array(x) \u001b[38;5;129;01mor\u001b[39;00m _is_tensorflow_array(x):\n\u001b[32m   2367\u001b[39m     \u001b[38;5;66;03m# using np.asarray() instead of explicitly __array__(), as the latter is\u001b[39;00m\n\u001b[32m   2368\u001b[39m     \u001b[38;5;66;03m# only _one_ of many methods, and it's the last resort, see also\u001b[39;00m\n\u001b[32m   2369\u001b[39m     \u001b[38;5;66;03m# https://numpy.org/devdocs/user/basics.interoperability.html#using-arbitrary-objects-in-numpy\u001b[39;00m\n\u001b[32m   2370\u001b[39m     \u001b[38;5;66;03m# therefore, let arrays do better if they can\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m     xtmp = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2373\u001b[39m     \u001b[38;5;66;03m# In case np.asarray method does not return a numpy array in future\u001b[39;00m\n\u001b[32m   2374\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(xtmp, np.ndarray):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\pytorch-course-revise\\.venv\\Lib\\site-packages\\torch\\_tensor.py:1194\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    \n",
    "    plot_predictions(predictions=y_pred.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8188c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
